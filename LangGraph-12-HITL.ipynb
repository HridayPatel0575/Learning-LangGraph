{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f2f1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama,OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, BaseMessage,AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from langgraph.graph import StateGraph, START,END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from typing import Annotated, TypedDict,List\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d502e04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972c3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model='qwen2:7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce81d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatState(TypedDict):\n",
    "\n",
    "    messages: Annotated[list[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2278b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "\n",
    "    decision = interrupt({\n",
    "        \"type\": \"approval\",\n",
    "        \"reason\": \"Model is about to answer a user question.\",\n",
    "        \"question\": state[\"messages\"][-1].content,\n",
    "        \"instruction\": \"Approve this question? yes/no\"\n",
    "    })\n",
    "    \n",
    "    if decision[\"approved\"] == 'no':\n",
    "        return {\"messages\": [AIMessage(content=\"Not approved.\")]}\n",
    "\n",
    "    else:\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847b50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ChatState)\n",
    "\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "workflow = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80860be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": '1'}}\n",
    "\n",
    "initial_input = {\n",
    "        \"messages\":[(\"user\", \"Explain gradient descent in very simple terms.\")]\n",
    "}\n",
    "\n",
    "result = workflow.invoke(initial_input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7205224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'approval',\n",
       " 'reason': 'Model is about to answer a user question.',\n",
       " 'question': 'Explain gradient descent in very simple terms.',\n",
       " 'instruction': 'Approve this question? yes/no'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = result['__interrupt__'][0].value\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c66e05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(f\"\\nBackend message - {message} \\n Approve this question? (y/n): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e3813ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = workflow.invoke(\n",
    "    Command(resume={\"approved\": user_input}),\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bec60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent is an optimization algorithm used to find the best parameters (such as coefficients or weights) for a model so that it minimizes a specific cost function. In simpler terms, it helps us find the lowest point in a valley by looking at the slope of the terrain around us.\n",
      "\n",
      "Imagine you're standing on a hilly landscape and trying to find the lowest point. Gradient descent works like this:\n",
      "\n",
      "1. **Choose an initial starting point**: This represents your first guess for where the lowest point might be.\n",
      "2. **Look around**: At your current position, check the slope of the hill (this is called the gradient). The direction with the most negative slope indicates downhill and thus toward the lowest point.\n",
      "3. **Move downhill**: Take a small step in that downhill direction. This new position becomes your next guess for where the lowest point might be.\n",
      "4. **Repeat steps 2 & 3**: Continue checking the slope around you (the gradient) and moving in the downhill direction until you can't find a steeper slope anymore or have reached a flat area, which should ideally be near the lowest point.\n",
      "\n",
      "This process of \"moving downhill\" is repeated over and over until we either reach the bottom of the valley (local minimum), or we've found the lowest point within our tolerance level (convergence).\n",
      "\n",
      "In machine learning contexts, this method helps to minimize errors between predicted values by a model and actual data points, leading to better predictions.\n"
     ]
    }
   ],
   "source": [
    "print(final_result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genvenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
